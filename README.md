# Road to LLM

A learning roadmap from the [tensor][1] to [large language models (LLMs)][2].

Inspired by [fromthetensor][3] and [ai-notebooks][4].

## Quickstart Guide

A [virtual environment][5] is highly recommended.

```
python3 -m venv env
source env/bin/activate
pip install -e .
```

## Roadmap

### Section 1: Introduction

- [Introduction to PyTorch][6]
- [Building Models with PyTorch][7]
- [Training with PyTorch][8]

### Section 2: Vision

- [Deep Learning in Neural Networks (2014)][9]
- [An Introduction to Convolutional Neural Networks(2015)][10]
- [Deep Residual Learning for Image Recognition (2015)][11]
- [Transformers for Image Recognition at Scale (2020)][12]

### Section 3: Language

- [Attention Is All You Need (2017)][13]
- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (2019)][14]
- [Language Models are Unsupervised Multitask Learners (2019)][15]
- [Language Models are Few-Shot Learners (2020)][16]
- [LLaMA: Open and Efficient Foundation Language Models (2023)][17]

[1]: https://en.wikipedia.org/wiki/Tensor
[2]: https://en.wikipedia.org/wiki/Large_language_model
[3]: https://github.com/jla524/fromthetensor
[4]: https://github.com/geohot/ai-notebooks
[5]: https://www.freecodecamp.org/news/how-to-setup-virtual-environments-in-python/
[6]: https://pytorch.org/tutorials/beginner/introyt/introyt1_tutorial.html
[7]: https://pytorch.org/tutorials/beginner/introyt/modelsyt_tutorial.html
[8]: https://pytorch.org/tutorials/beginner/introyt/trainingyt.html
[9]: https://arxiv.org/abs/1404.7828
[10]: https://arxiv.org/abs/1511.08458
[11]: https://arxiv.org/abs/1512.03385
[12]: https://arxiv.org/abs/2010.11929
[13]: https://arxiv.org/abs/1706.03762
[14]: https://arxiv.org/abs/1810.04805
[15]: https://paperswithcode.com/paper/language-models-are-unsupervised-multitask
[16]: https://arxiv.org/abs/2005.14165
[17]: https://arxiv.org/abs/2302.13971
