# Road to LLM

A learning roadmap from the [tensor][1] to [large language models (LLMs)][2].

Inspired by [fromthetensor][3] and [ai-notebooks][4].

## Quickstart Guide

A [virtual environment][5] is highly recommended.

```
python3 -m venv env
source env/bin/activate
pip install -e .
```

## Roadmap

### Section 1: Introduction

- [Introduction to tinygrad][6]

### Section 2: Vision

- [Deep Learning in Neural Networks (2014)][7]
- [An Introduction to Convolutional Neural Networks(2015)][8]
- [Deep Residual Learning for Image Recognition (2015)][9]
- [Transformers for Image Recognition at Scale (2020)][10]

### Section 3: Language

- [Attention Is All You Need (2017)][11]
- [Language Models are Unsupervised Multitask Learners (2019)][12]
- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (2019)][13]
- [Language Models are Few-Shot Learners (2020)][14]
- [LLaMA: Open and Efficient Foundation Language Models (2023)][15]

[1]: https://en.wikipedia.org/wiki/Tensor
[2]: https://en.wikipedia.org/wiki/Large_language_model
[3]: https://github.com/jla524/fromthetensor
[4]: https://github.com/geohot/ai-notebooks
[5]: https://www.freecodecamp.org/news/how-to-setup-virtual-environments-in-python/
[6]: https://github.com/tinygrad/tinygrad/tree/master/docs
[7]: https://arxiv.org/abs/1404.7828
[8]: https://arxiv.org/abs/1511.08458
[9]: https://arxiv.org/abs/1512.03385
[10]: https://arxiv.org/abs/2010.11929
[11]: https://arxiv.org/abs/1706.03762
[12]: https://paperswithcode.com/paper/language-models-are-unsupervised-multitask
[13]: https://arxiv.org/abs/1810.04805
[14]: https://arxiv.org/abs/2005.14165
[15]: https://arxiv.org/abs/2302.13971
